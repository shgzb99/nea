{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import scipy\n",
    "from time import time\n",
    "import sys\n",
    "import nea.utils as U\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = r'C:\\Graphviz\\bin;' + os.environ['PATH']\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Parse arguments\n",
    "#\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-tr\", \"--train\", dest=\"train_path\", type=str, metavar='<str>', required=True, help=\"The path to the training set\")\n",
    "parser.add_argument(\"-tu\", \"--tune\", dest=\"dev_path\", type=str, metavar='<str>', required=True, help=\"The path to the development set\")\n",
    "parser.add_argument(\"-ts\", \"--test\", dest=\"test_path\", type=str, metavar='<str>', required=True, help=\"The path to the test set\")\n",
    "parser.add_argument(\"-o\", \"--out-dir\", dest=\"out_dir_path\", type=str, metavar='<str>', required=True, help=\"The path to the output directory\")\n",
    "parser.add_argument(\"-p\", \"--prompt\", dest=\"prompt_id\", type=int, metavar='<int>', required=False, help=\"Promp ID for ASAP dataset. '0' means all prompts.\")\n",
    "parser.add_argument(\"-t\", \"--type\", dest=\"model_type\", type=str, metavar='<str>', default='regp', help=\"Model type (reg|regp|breg|bregp) (default=regp)\")\n",
    "parser.add_argument(\"-u\", \"--rec-unit\", dest=\"recurrent_unit\", type=str, metavar='<str>', default='lstm', help=\"Recurrent unit type (lstm|gru|simple) (default=lstm)\")\n",
    "parser.add_argument(\"-a\", \"--algorithm\", dest=\"algorithm\", type=str, metavar='<str>', default='rmsprop', help=\"Optimization algorithm (rmsprop|sgd|adagrad|adadelta|adam|adamax) (default=rmsprop)\")\n",
    "parser.add_argument(\"-l\", \"--loss\", dest=\"loss\", type=str, metavar='<str>', default='mse', help=\"Loss function (mse|mae) (default=mse)\")\n",
    "parser.add_argument(\"-e\", \"--embdim\", dest=\"emb_dim\", type=int, metavar='<int>', default=50, help=\"Embeddings dimension (default=50)\")\n",
    "parser.add_argument(\"-c\", \"--cnndim\", dest=\"cnn_dim\", type=int, metavar='<int>', default=0, help=\"CNN output dimension. '0' means no CNN layer (default=0)\")\n",
    "parser.add_argument(\"-w\", \"--cnnwin\", dest=\"cnn_window_size\", type=int, metavar='<int>', default=3, help=\"CNN window size. (default=3)\")\n",
    "parser.add_argument(\"-r\", \"--rnndim\", dest=\"rnn_dim\", type=int, metavar='<int>', default=300, help=\"RNN dimension. '0' means no RNN layer (default=300)\")\n",
    "parser.add_argument(\"-b\", \"--batch-size\", dest=\"batch_size\", type=int, metavar='<int>', default=32, help=\"Batch size (default=32)\")\n",
    "parser.add_argument(\"-v\", \"--vocab-size\", dest=\"vocab_size\", type=int, metavar='<int>', default=4000, help=\"Vocab size (default=4000)\")\n",
    "parser.add_argument(\"--aggregation\", dest=\"aggregation\", type=str, metavar='<str>', default='mot', help=\"The aggregation method for regp and bregp types (mot|attsum|attmean) (default=mot)\")\n",
    "parser.add_argument(\"--dropout\", dest=\"dropout_prob\", type=float, metavar='<float>', default=0.5, help=\"The dropout probability. To disable, give a negative number (default=0.5)\")\n",
    "parser.add_argument(\"--vocab-path\", dest=\"vocab_path\", type=str, metavar='<str>', help=\"(Optional) The path to the existing vocab file (*.pkl)\")\n",
    "parser.add_argument(\"--skip-init-bias\", dest=\"skip_init_bias\", action='store_true', help=\"Skip initialization of the last layer bias\")\n",
    "parser.add_argument(\"--emb\", dest=\"emb_path\", type=str, metavar='<str>', help=\"The path to the word embeddings file (Word2Vec format)\")\n",
    "parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, metavar='<int>', default=50, help=\"Number of epochs (default=50)\")\n",
    "parser.add_argument(\"--maxlen\", dest=\"maxlen\", type=int, metavar='<int>', default=0, help=\"Maximum allowed number of words during training. '0' means no limit (default=0)\")\n",
    "parser.add_argument(\"--seed\", dest=\"seed\", type=int, metavar='<int>', default=1234, help=\"Random seed (default=1234)\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "out_dir = args.out_dir_path\n",
    "\n",
    "U.mkdir_p(out_dir + '/preds')\n",
    "U.set_logger(out_dir)\n",
    "U.print_args(args)\n",
    "\n",
    "assert args.model_type in {'reg', 'regp', 'breg', 'bregp'}\n",
    "assert args.algorithm in {'rmsprop', 'sgd', 'adagrad', 'adadelta', 'adam', 'adamax'}\n",
    "assert args.loss in {'mse', 'mae'}\n",
    "assert args.recurrent_unit in {'lstm', 'gru', 'simple'}\n",
    "assert args.aggregation in {'mot', 'attsum', 'attmean'}\n",
    "\n",
    "if args.seed > 0:\n",
    "\tnp.random.seed(args.seed)\n",
    "\n",
    "if args.prompt_id:\n",
    "\tfrom nea.asap_evaluator import Evaluator\n",
    "\timport nea.asap_reader as dataset\n",
    "else:\n",
    "\traise NotImplementedError\n",
    "\n",
    "###############################################################################################################################\n",
    "## Prepare data\n",
    "#\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# data_x is a list of lists\n",
    "(train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data(\n",
    "\t(args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path)\n",
    "\n",
    "# Dump vocab\n",
    "with open(out_dir + '/vocab.pkl', 'wb') as vocab_file:\n",
    "\tpk.dump(vocab, vocab_file)\n",
    "\n",
    "# Pad sequences for mini-batch processing\n",
    "if args.model_type in {'breg', 'bregp'}:\n",
    "\tassert args.rnn_dim > 0\n",
    "\tassert args.recurrent_unit == 'lstm'\n",
    "\ttrain_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)\n",
    "\tdev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)\n",
    "\ttest_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)\n",
    "else:\n",
    "\ttrain_x = sequence.pad_sequences(train_x)\n",
    "\tdev_x = sequence.pad_sequences(dev_x)\n",
    "\ttest_x = sequence.pad_sequences(test_x)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Some statistics\n",
    "#\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "train_y = np.array(train_y, dtype=K.floatx())\n",
    "dev_y = np.array(dev_y, dtype=K.floatx())\n",
    "test_y = np.array(test_y, dtype=K.floatx())\n",
    "\n",
    "if args.prompt_id:\n",
    "\ttrain_pmt = np.array(train_pmt, dtype='int32')\n",
    "\tdev_pmt = np.array(dev_pmt, dtype='int32')\n",
    "\ttest_pmt = np.array(test_pmt, dtype='int32')\n",
    "\n",
    "bincounts, mfs_list = U.bincounts(train_y)\n",
    "with open('%s/bincounts.txt' % out_dir, 'w') as output_file:\n",
    "\tfor bincount in bincounts:\n",
    "\t\toutput_file.write(str(bincount) + '\\n')\n",
    "\n",
    "train_mean = train_y.mean(axis=0)\n",
    "train_std = train_y.std(axis=0)\n",
    "dev_mean = dev_y.mean(axis=0)\n",
    "dev_std = dev_y.std(axis=0)\n",
    "test_mean = test_y.mean(axis=0)\n",
    "test_std = test_y.std(axis=0)\n",
    "\n",
    "logger.info('Statistics:')\n",
    "\n",
    "logger.info('  train_x shape: ' + str(np.array(train_x).shape))\n",
    "logger.info('  dev_x shape:   ' + str(np.array(dev_x).shape))\n",
    "logger.info('  test_x shape:  ' + str(np.array(test_x).shape))\n",
    "\n",
    "logger.info('  train_y shape: ' + str(train_y.shape))\n",
    "logger.info('  dev_y shape:   ' + str(dev_y.shape))\n",
    "logger.info('  test_y shape:  ' + str(test_y.shape))\n",
    "\n",
    "logger.info('  train_y mean: %s, stdev: %s, MFC: %s' % (str(train_mean), str(train_std), str(mfs_list)))\n",
    "\n",
    "# We need the dev and test sets in the original scale for evaluation\n",
    "dev_y_org = dev_y.astype(dataset.get_ref_dtype())\n",
    "test_y_org = test_y.astype(dataset.get_ref_dtype())\n",
    "\n",
    "# Convert scores to boundary of [0 1] for training and evaluation (loss calculation)\n",
    "train_y = dataset.get_model_friendly_scores(train_y, train_pmt)\n",
    "dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)\n",
    "test_y = dataset.get_model_friendly_scores(test_y, test_pmt)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Optimizaer algorithm\n",
    "#\n",
    "\n",
    "from nea.optimizers import get_optimizer\n",
    "\n",
    "optimizer = get_optimizer(args)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Building model\n",
    "#\n",
    "\n",
    "from nea.models import create_model\n",
    "\n",
    "if args.loss == 'mse':\n",
    "\tloss = 'mean_squared_error'\n",
    "\tmetric = 'mean_absolute_error'\n",
    "else:\n",
    "\tloss = 'mean_absolute_error'\n",
    "\tmetric = 'mean_squared_error'\n",
    "\n",
    "model = create_model(args, train_y.mean(axis=0), overal_maxlen, vocab)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "\n",
    "###############################################################################################################################\n",
    "## Plotting model\n",
    "#\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "\n",
    "plot(model, to_file = out_dir + '/model.png')\n",
    "\n",
    "###############################################################################################################################\n",
    "## Save model architecture\n",
    "#\n",
    "\n",
    "logger.info('Saving model architecture')\n",
    "with open(out_dir + '/model_arch.json', 'w') as arch:\n",
    "\tarch.write(model.to_json(indent=2))\n",
    "logger.info('  Done')\n",
    "\t\n",
    "###############################################################################################################################\n",
    "## Evaluator\n",
    "#\n",
    "\n",
    "evl = Evaluator(dataset, args.prompt_id, out_dir, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Training\n",
    "#\n",
    "\n",
    "logger.info('--------------------------------------------------------------------------------------------------------------------------')\n",
    "logger.info('Initial Evaluation:')\n",
    "evl.evaluate(model, -1, print_info=True)\n",
    "\n",
    "total_train_time = 0\n",
    "total_eval_time = 0\n",
    "\n",
    "for ii in range(args.epochs):\n",
    "\t# Training\n",
    "\tt0 = time()\n",
    "\ttrain_history = model.fit(train_x, train_y, batch_size=args.batch_size, nb_epoch=1, verbose=0)\n",
    "\ttr_time = time() - t0\n",
    "\ttotal_train_time += tr_time\n",
    "\t\n",
    "\t# Evaluate\n",
    "\tt0 = time()\n",
    "\tevl.evaluate(model, ii)\n",
    "\tevl_time = time() - t0\n",
    "\ttotal_eval_time += evl_time\n",
    "\t\n",
    "\t# Print information\n",
    "\ttrain_loss = train_history.history['loss'][0]\n",
    "\ttrain_metric = train_history.history[metric][0]\n",
    "\tlogger.info('Epoch %d, train: %is, evaluation: %is' % (ii, tr_time, evl_time))\n",
    "\tlogger.info('[Train] loss: %.4f, metric: %.4f' % (train_loss, train_metric))\n",
    "\tevl.print_info()\n",
    "\n",
    "###############################################################################################################################\n",
    "## Summary of the results\n",
    "#\n",
    "\n",
    "logger.info('Training:   %i seconds in total' % total_train_time)\n",
    "logger.info('Evaluation: %i seconds in total' % total_eval_time)\n",
    "\n",
    "evl.print_final_info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import scipy\n",
    "from time import time\n",
    "import sys\n",
    "import nea.utils as U\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = r'C:\\Graphviz\\bin;' + os.environ['PATH']\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Parse arguments\n",
    "#\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-tr\", \"--train\", dest=\"train_path\", type=str, metavar='<str>', required=True, help=\"The path to the training set\")\n",
    "parser.add_argument(\"-tu\", \"--tune\", dest=\"dev_path\", type=str, metavar='<str>', required=True, help=\"The path to the development set\")\n",
    "parser.add_argument(\"-ts\", \"--test\", dest=\"test_path\", type=str, metavar='<str>', required=True, help=\"The path to the test set\")\n",
    "parser.add_argument(\"-o\", \"--out-dir\", dest=\"out_dir_path\", type=str, metavar='<str>', required=True, help=\"The path to the output directory\")\n",
    "parser.add_argument(\"-p\", \"--prompt\", dest=\"prompt_id\", type=int, metavar='<int>', required=False, help=\"Promp ID for ASAP dataset. '0' means all prompts.\")\n",
    "parser.add_argument(\"-t\", \"--type\", dest=\"model_type\", type=str, metavar='<str>', default='regp', help=\"Model type (reg|regp|breg|bregp) (default=regp)\")\n",
    "parser.add_argument(\"-u\", \"--rec-unit\", dest=\"recurrent_unit\", type=str, metavar='<str>', default='lstm', help=\"Recurrent unit type (lstm|gru|simple) (default=lstm)\")\n",
    "parser.add_argument(\"-a\", \"--algorithm\", dest=\"algorithm\", type=str, metavar='<str>', default='rmsprop', help=\"Optimization algorithm (rmsprop|sgd|adagrad|adadelta|adam|adamax) (default=rmsprop)\")\n",
    "parser.add_argument(\"-l\", \"--loss\", dest=\"loss\", type=str, metavar='<str>', default='mse', help=\"Loss function (mse|mae) (default=mse)\")\n",
    "parser.add_argument(\"-e\", \"--embdim\", dest=\"emb_dim\", type=int, metavar='<int>', default=50, help=\"Embeddings dimension (default=50)\")\n",
    "parser.add_argument(\"-c\", \"--cnndim\", dest=\"cnn_dim\", type=int, metavar='<int>', default=0, help=\"CNN output dimension. '0' means no CNN layer (default=0)\")\n",
    "parser.add_argument(\"-w\", \"--cnnwin\", dest=\"cnn_window_size\", type=int, metavar='<int>', default=3, help=\"CNN window size. (default=3)\")\n",
    "parser.add_argument(\"-r\", \"--rnndim\", dest=\"rnn_dim\", type=int, metavar='<int>', default=300, help=\"RNN dimension. '0' means no RNN layer (default=300)\")\n",
    "parser.add_argument(\"-b\", \"--batch-size\", dest=\"batch_size\", type=int, metavar='<int>', default=32, help=\"Batch size (default=32)\")\n",
    "parser.add_argument(\"-v\", \"--vocab-size\", dest=\"vocab_size\", type=int, metavar='<int>', default=4000, help=\"Vocab size (default=4000)\")\n",
    "parser.add_argument(\"--aggregation\", dest=\"aggregation\", type=str, metavar='<str>', default='mot', help=\"The aggregation method for regp and bregp types (mot|attsum|attmean) (default=mot)\")\n",
    "parser.add_argument(\"--dropout\", dest=\"dropout_prob\", type=float, metavar='<float>', default=0.5, help=\"The dropout probability. To disable, give a negative number (default=0.5)\")\n",
    "parser.add_argument(\"--vocab-path\", dest=\"vocab_path\", type=str, metavar='<str>', help=\"(Optional) The path to the existing vocab file (*.pkl)\")\n",
    "parser.add_argument(\"--skip-init-bias\", dest=\"skip_init_bias\", action='store_true', help=\"Skip initialization of the last layer bias\")\n",
    "parser.add_argument(\"--emb\", dest=\"emb_path\", type=str, metavar='<str>', help=\"The path to the word embeddings file (Word2Vec format)\")\n",
    "parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, metavar='<int>', default=50, help=\"Number of epochs (default=50)\")\n",
    "parser.add_argument(\"--maxlen\", dest=\"maxlen\", type=int, metavar='<int>', default=0, help=\"Maximum allowed number of words during training. '0' means no limit (default=0)\")\n",
    "parser.add_argument(\"--seed\", dest=\"seed\", type=int, metavar='<int>', default=1234, help=\"Random seed (default=1234)\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "out_dir = args.out_dir_path\n",
    "\n",
    "U.mkdir_p(out_dir + '/preds')\n",
    "U.set_logger(out_dir)\n",
    "U.print_args(args)\n",
    "\n",
    "assert args.model_type in {'reg', 'regp', 'breg', 'bregp'}\n",
    "assert args.algorithm in {'rmsprop', 'sgd', 'adagrad', 'adadelta', 'adam', 'adamax'}\n",
    "assert args.loss in {'mse', 'mae'}\n",
    "assert args.recurrent_unit in {'lstm', 'gru', 'simple'}\n",
    "assert args.aggregation in {'mot', 'attsum', 'attmean'}\n",
    "\n",
    "if args.seed > 0:\n",
    "\tnp.random.seed(args.seed)\n",
    "\n",
    "if args.prompt_id:\n",
    "\tfrom nea.asap_evaluator import Evaluator\n",
    "\timport nea.asap_reader as dataset\n",
    "else:\n",
    "\traise NotImplementedError\n",
    "\n",
    "###############################################################################################################################\n",
    "## Prepare data\n",
    "#\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# data_x is a list of lists\n",
    "(train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data(\n",
    "\t(args.train_path, args.dev_path, args.test_path), args.prompt_id, args.vocab_size, args.maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=args.vocab_path)\n",
    "\n",
    "# Dump vocab\n",
    "with open(out_dir + '/vocab.pkl', 'wb') as vocab_file:\n",
    "\tpk.dump(vocab, vocab_file)\n",
    "\n",
    "# Pad sequences for mini-batch processing\n",
    "if args.model_type in {'breg', 'bregp'}:\n",
    "\tassert args.rnn_dim > 0\n",
    "\tassert args.recurrent_unit == 'lstm'\n",
    "\ttrain_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)\n",
    "\tdev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)\n",
    "\ttest_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)\n",
    "else:\n",
    "\ttrain_x = sequence.pad_sequences(train_x)\n",
    "\tdev_x = sequence.pad_sequences(dev_x)\n",
    "\ttest_x = sequence.pad_sequences(test_x)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Some statistics\n",
    "#\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "train_y = np.array(train_y, dtype=K.floatx())\n",
    "dev_y = np.array(dev_y, dtype=K.floatx())\n",
    "test_y = np.array(test_y, dtype=K.floatx())\n",
    "\n",
    "if args.prompt_id:\n",
    "\ttrain_pmt = np.array(train_pmt, dtype='int32')\n",
    "\tdev_pmt = np.array(dev_pmt, dtype='int32')\n",
    "\ttest_pmt = np.array(test_pmt, dtype='int32')\n",
    "\n",
    "bincounts, mfs_list = U.bincounts(train_y)\n",
    "with open('%s/bincounts.txt' % out_dir, 'w') as output_file:\n",
    "\tfor bincount in bincounts:\n",
    "\t\toutput_file.write(str(bincount) + '\\n')\n",
    "\n",
    "train_mean = train_y.mean(axis=0)\n",
    "train_std = train_y.std(axis=0)\n",
    "dev_mean = dev_y.mean(axis=0)\n",
    "dev_std = dev_y.std(axis=0)\n",
    "test_mean = test_y.mean(axis=0)\n",
    "test_std = test_y.std(axis=0)\n",
    "\n",
    "logger.info('Statistics:')\n",
    "\n",
    "logger.info('  train_x shape: ' + str(np.array(train_x).shape))\n",
    "logger.info('  dev_x shape:   ' + str(np.array(dev_x).shape))\n",
    "logger.info('  test_x shape:  ' + str(np.array(test_x).shape))\n",
    "\n",
    "logger.info('  train_y shape: ' + str(train_y.shape))\n",
    "logger.info('  dev_y shape:   ' + str(dev_y.shape))\n",
    "logger.info('  test_y shape:  ' + str(test_y.shape))\n",
    "\n",
    "logger.info('  train_y mean: %s, stdev: %s, MFC: %s' % (str(train_mean), str(train_std), str(mfs_list)))\n",
    "\n",
    "# We need the dev and test sets in the original scale for evaluation\n",
    "dev_y_org = dev_y.astype(dataset.get_ref_dtype())\n",
    "test_y_org = test_y.astype(dataset.get_ref_dtype())\n",
    "\n",
    "# Convert scores to boundary of [0 1] for training and evaluation (loss calculation)\n",
    "train_y = dataset.get_model_friendly_scores(train_y, train_pmt)\n",
    "dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)\n",
    "test_y = dataset.get_model_friendly_scores(test_y, test_pmt)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Optimizaer algorithm\n",
    "#\n",
    "\n",
    "from nea.optimizers import get_optimizer\n",
    "\n",
    "optimizer = get_optimizer(args)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Building model\n",
    "#\n",
    "\n",
    "from nea.models import create_model\n",
    "\n",
    "if args.loss == 'mse':\n",
    "\tloss = 'mean_squared_error'\n",
    "\tmetric = 'mean_absolute_error'\n",
    "else:\n",
    "\tloss = 'mean_absolute_error'\n",
    "\tmetric = 'mean_squared_error'\n",
    "\n",
    "model = create_model(args, train_y.mean(axis=0), overal_maxlen, vocab)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "\n",
    "###############################################################################################################################\n",
    "## Plotting model\n",
    "#\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "\n",
    "plot(model, to_file = out_dir + '/model.png')\n",
    "\n",
    "###############################################################################################################################\n",
    "## Save model architecture\n",
    "#\n",
    "\n",
    "logger.info('Saving model architecture')\n",
    "with open(out_dir + '/model_arch.json', 'w') as arch:\n",
    "\tarch.write(model.to_json(indent=2))\n",
    "logger.info('  Done')\n",
    "\t\n",
    "###############################################################################################################################\n",
    "## Evaluator\n",
    "#\n",
    "\n",
    "evl = Evaluator(dataset, args.prompt_id, out_dir, dev_x, test_x, dev_y, test_y, dev_y_org, test_y_org)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Training\n",
    "#\n",
    "\n",
    "logger.info('--------------------------------------------------------------------------------------------------------------------------')\n",
    "logger.info('Initial Evaluation:')\n",
    "evl.evaluate(model, -1, print_info=True)\n",
    "\n",
    "total_train_time = 0\n",
    "total_eval_time = 0\n",
    "\n",
    "for ii in range(args.epochs):\n",
    "\t# Training\n",
    "\tt0 = time()\n",
    "\ttrain_history = model.fit(train_x, train_y, batch_size=args.batch_size, nb_epoch=1, verbose=0)\n",
    "\ttr_time = time() - t0\n",
    "\ttotal_train_time += tr_time\n",
    "\t\n",
    "\t# Evaluate\n",
    "\tt0 = time()\n",
    "\tevl.evaluate(model, ii)\n",
    "\tevl_time = time() - t0\n",
    "\ttotal_eval_time += evl_time\n",
    "\t\n",
    "\t# Print information\n",
    "\ttrain_loss = train_history.history['loss'][0]\n",
    "\ttrain_metric = train_history.history[metric][0]\n",
    "\tlogger.info('Epoch %d, train: %is, evaluation: %is' % (ii, tr_time, evl_time))\n",
    "\tlogger.info('[Train] loss: %.4f, metric: %.4f' % (train_loss, train_metric))\n",
    "\tevl.print_info()\n",
    "\n",
    "###############################################################################################################################\n",
    "## Summary of the results\n",
    "#\n",
    "\n",
    "logger.info('Training:   %i seconds in total' % total_train_time)\n",
    "logger.info('Evaluation: %i seconds in total' % total_eval_time)\n",
    "\n",
    "evl.print_final_info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
